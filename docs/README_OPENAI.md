# StudyMate - OpenAI Integration Setup

## √úbersicht

Die App wurde so umgebaut, dass alle LLM-Anfragen √ºber **deine eigene OpenAI-API** laufen, nicht mehr √ºber GitHub Models/Spark Tools.

## Architektur

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend  ‚îÇ  fetch  ‚îÇ   Backend   ‚îÇ   API   ‚îÇ   OpenAI    ‚îÇ
‚îÇ  (React)    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  (Express)  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ             ‚îÇ
‚îÇ  Port 5000  ‚îÇ         ‚îÇ  Port 3001  ‚îÇ         ‚îÇ gpt-4o-mini ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Warum diese Architektur?

- **Sicherheit**: Der OpenAI API-Key bleibt ausschlie√ülich auf dem Server, niemals im Browser
- **Kontrolle**: Du kannst das Modell zentral wechseln, Logging hinzuf√ºgen, Kosten √ºberwachen
- **Rate Limiting**: Bessere Kontrolle √ºber API-Aufrufe und Fehlerbehandlung

## Setup-Anleitung

### 1. OpenAI API-Key erhalten

1. Gehe zu [OpenAI Platform](https://platform.openai.com/api-keys)
2. Melde dich an oder erstelle einen Account
3. Navigiere zu "API Keys"
4. Klicke auf "Create new secret key"
5. Kopiere den Key (du siehst ihn nur einmal!)

### 2. Umgebungsvariablen konfigurieren

Erstelle eine `.env`-Datei im Projekt-Root:

```bash
cp .env.example .env
```

√ñffne `.env` und f√ºge deinen API-Key ein:

```env
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
PORT=3001
```

‚ö†Ô∏è **WICHTIG**: Die `.env`-Datei ist in `.gitignore` und wird niemals commited!

### 3. Dependencies installieren

```bash
npm install
```

### 4. Beide Server starten

**Option A - Alles zusammen (empfohlen):**

```bash
npm run dev:full
```

**Option B - Einzeln in separaten Terminals:**

Terminal 1 (Backend):

```bash
npm run server
```

Terminal 2 (Frontend):

```bash
npm run dev
```

### 5. Verifizierung

√ñffne http://localhost:5000 - die App sollte funktionieren!

Um zu testen, ob der Backend-Server l√§uft:

```bash
curl http://localhost:3001/api/health
```

Erwartete Antwort:

```json
{
  "status": "ok",
  "timestamp": "2024-01-15T10:30:00.000Z",
  "apiKeyConfigured": true
}
```

## Ge√§nderte Dateien

### Neu erstellt:

- `server/index.js` - Express Backend mit OpenAI Integration
- `.env.example` - Beispiel f√ºr Umgebungsvariablen
- `README_OPENAI.md` - Diese Datei

### Modifiziert:

- `src/lib/llm-utils.ts` - Ersetzt `spark.llm()` durch `fetch()` zu Backend
- `package.json` - Neue Scripts und Dependencies (openai, express, cors, concurrently)

### Unver√§ndert (funktionieren weiterhin):

- Alle UI-Komponenten
- Rate Limiting Logik
- Task Queue System
- Debug Console
- Alle anderen Features

## Modell-Konfiguration

### Standard-Modell √§ndern

In `server/index.js`, Zeile 9:

```javascript
const DEFAULT_MODEL = "gpt-4o-mini"; // ‚Üê Hier √§ndern
```

Verf√ºgbare Modelle (Stand Jan 2024):

- `gpt-4o-mini` - Schnell, g√ºnstig, gut f√ºr die meisten Aufgaben (Standard)
- `gpt-4o` - Leistungsst√§rker, teurer
- `gpt-4-turbo` - Hohe Qualit√§t
- `gpt-3.5-turbo` - G√ºnstigste Option

### Modell pro Anfrage w√§hlen

Im Frontend wird das Modell bei `llmWithRetry()` √ºbergeben:

```typescript
// In App.tsx, z.B. Zeile 185
const response = await llmWithRetry(prompt, "gpt-4o-mini", false);
```

## Fehlerbehandlung

### Backend meldet klare Fehler:

| HTTP Status | Fehler       | Bedeutung                           |
| ----------- | ------------ | ----------------------------------- |
| 400         | Bad Request  | Prompt fehlt im Request             |
| 429         | Rate Limit   | Zu viele Anfragen an OpenAI         |
| 413         | Token Limit  | Text ist zu lang f√ºr das Modell     |
| 500         | Server Error | API-Key ung√ºltig oder nicht gesetzt |

### Retry-Logik

- Standardm√§√üig **1 Retry** bei Fehlern
- Exponentielles Backoff (2s, 4s, 8s...)
- Bei 429: 5 Minuten Cooldown automatisch aktiviert

## Kosten-√úbersicht

OpenAI berechnet nach Token-Verbrauch. Ungef√§hre Kosten (Stand Jan 2024):

| Modell        | Input (pro 1M Tokens) | Output (pro 1M Tokens) |
| ------------- | --------------------- | ---------------------- |
| gpt-4o-mini   | $0.15                 | $0.60                  |
| gpt-4o        | $2.50                 | $10.00                 |
| gpt-3.5-turbo | $0.50                 | $1.50                  |

**Beispiel**: Eine Notizgenerierung (~2000 Input + 500 Output Tokens) mit gpt-4o-mini kostet ca. $0.0006

## Debugging

### Backend-Logs

Der Server zeigt alle Requests:

```
[LLM] Request - Model: gpt-4o-mini, JSON Mode: false, Prompt length: 1234
[LLM] Response - Length: 567, Tokens: 234
```

### Frontend Debug-Konsole

Aktiviere den Debug-Modus in der App (Toggle oben rechts):

- Zeigt alle Prompts
- Zeigt alle Responses
- Zeigt Token-Verbrauch
- Zeigt Fehler mit Stack Trace

### Health Check

```bash
# Backend Status
curl http://localhost:3001/api/health

# Test LLM Request
curl -X POST http://localhost:3001/api/llm \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Was ist 2+2?", "model": "gpt-4o-mini"}'
```

## H√§ufige Probleme

### "Verbindung zum Backend fehlgeschlagen"

**L√∂sung**: Stelle sicher, dass der Backend-Server l√§uft:

```bash
npm run server
```

### "OPENAI_API_KEY nicht gesetzt"

**L√∂sung**: Erstelle `.env`-Datei mit deinem API-Key:

```bash
echo "OPENAI_API_KEY=sk-..." > .env
```

### "Rate Limit erreicht"

**L√∂sung**:

1. Warte 5 Minuten (automatischer Cooldown)
2. √úberpr√ºfe dein OpenAI-Kontingent: https://platform.openai.com/usage
3. Evtl. zu `gpt-3.5-turbo` wechseln (h√∂here Limits)

### "Token-Limit √ºberschritten"

**L√∂sung**: Das Dokument ist zu gro√ü. Optionen:

1. Teile das Dokument in kleinere Abschnitte
2. Wechsle zu einem Modell mit gr√∂√üerem Context Window (`gpt-4-turbo`: 128k Tokens)

## Entwicklung

### Neue LLM-Features hinzuf√ºgen

1. **Im Frontend** (`src/lib/llm-utils.ts`):

```typescript
const response = await llmWithRetry(
  prompt,
  "gpt-4o-mini", // Modell
  false, // JSON Mode
  1 // Max Retries
);
```

2. **Backend erweitern** (`server/index.js`):

```javascript
// Z.B. Vision Support hinzuf√ºgen:
if (req.body.image) {
  requestOptions.messages[0].content = [
    { type: "text", text: prompt },
    { type: "image_url", image_url: { url: req.body.image } },
  ];
}
```

## Migration von GitHub Models

Alle Stellen, die vorher `spark.llm()` verwendet haben, nutzen jetzt:

1. `llmWithRetry()` im Frontend
2. Diese Funktion ruft `/api/llm` auf
3. Das Backend ruft OpenAI auf

**Keine √Ñnderungen n√∂tig** in:

- `App.tsx` - verwendet bereits `llmWithRetry()`
- Komponenten - keine direkten LLM-Calls
- UI-Logik - bleibt gleich

## Produktiv-Deployment

### Option 1: Gleicher Server (z.B. Vercel, Railway)

Stelle sicher, dass:

1. Beide Server starten: `npm run dev:full` (oder separates Prozess-Management)
2. `OPENAI_API_KEY` als Umgebungsvariable gesetzt ist
3. `VITE_API_URL` auf die Backend-URL zeigt

### Option 2: Separates Backend-Deployment

1. Backend auf separatem Service (z.B. Railway, Render, Fly.io)
2. Im Frontend `.env`:

```env
VITE_API_URL=https://your-backend-url.com
```

---

## üöÄ Render Deployment (Produktion)

### Backend bei Render hosten

Die App ist vorbereitet f√ºr Deployment bei [Render](https://render.com):

#### 1. Render-Projekt erstellen

1. Gehe zu [Render Dashboard](https://dashboard.render.com)
2. Klicke auf **"New +"** ‚Üí **"Blueprint"**
3. Verbinde dein GitHub Repository: `Butros55/studysync-ai-study-c`
4. Render erkennt die `render.yaml` automatisch und erstellt den Service

#### 2. OpenAI API-Key konfigurieren

Nach dem Deployment:

1. Gehe zu deinem Service im Render Dashboard
2. Navigiere zu **"Environment"**
3. Finde `OPENAI_API_KEY` und klicke auf **"Set Value"**
4. F√ºge deinen OpenAI API-Key ein
5. Klicke auf **"Save Changes"** - der Service startet automatisch neu

#### 3. Frontend f√ºr Produktion bauen

Aktualisiere `.env.production` mit deiner Render-URL:

```env
VITE_API_URL=https://studysync-backend.onrender.com
```

Dann baue das Frontend:

```bash
npm run build
```

Das Build-Ergebnis in `/dist` kann auf GitHub Pages deployed werden.

#### 4. GitHub Pages Deployment

Der GitHub Actions Workflow (`.github/workflows/deploy.yml`) baut das Frontend automatisch.
Stelle sicher, dass `VITE_API_URL` in `.env.production` auf deine Render-URL zeigt.

### Healthcheck testen

Nach dem Deployment:

```bash
curl https://studysync-backend.onrender.com/api/health
```

Erwartete Antwort:

```json
{
  "status": "ok",
  "timestamp": "2024-12-05T10:30:00.000Z",
  "environment": "production"
}
```

### ‚ö†Ô∏è Render Free Tier Hinweise

- Der Service geht nach 15 Minuten Inaktivit√§t in den Schlafmodus
- Der erste Request nach dem Aufwachen dauert ~30 Sekunden
- F√ºr bessere Performance: Upgrade auf kostenpflichtigen Plan

---

## Support

Bei Fragen oder Problemen:

1. √úberpr√ºfe die Logs in Backend (`npm run server`) und Browser-Console
2. Aktiviere Debug-Modus in der App (zeigt aktuelle API-URL)
3. √úberpr√ºfe OpenAI Status: https://status.openai.com
4. Pr√ºfe Render Logs: Dashboard ‚Üí Service ‚Üí "Logs"

## Sicherheit

‚úÖ **Richtig implementiert:**

- API-Key nur auf Server (Render ENV)
- CORS konfiguriert f√ºr localhost + GitHub Pages
- `.env` in `.gitignore`
- Input-Validierung im Backend
- Healthcheck-Route f√ºr Monitoring

‚ö†Ô∏è **F√ºr Produktion zus√§tzlich:**

- Rate Limiting pro User
- Request-Gr√∂√üen-Limit
- API-Key-Rotation
- Monitoring & Alerting
- Cost Limits in OpenAI Dashboard setzen
